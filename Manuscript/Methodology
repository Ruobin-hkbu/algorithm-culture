Methodology

This study employed a mixed-methods experimental design to investigate the effectiveness of different ethical alignment approaches in AI-generated advertisement content. We developed three distinct experimental conditions representing progressively sophisticated ethical frameworks, generated content across multiple demographic targets, and evaluated outputs using both quantitative systematic scoring and qualitative human expert assessment.

We utilized a comparative three-room experimental framework to test ethical alignment strategies. Room A served as the baseline control, employing minimal ethical guidance with standard copywriting instructions focused on persuasiveness and brand alignment. This condition established baseline metrics for unguided AI advertisement generation, allowing us to document the frequency and types of bias, stereotypes, cultural insensitivity, and manipulative tactics that emerge without systematic intervention. Room B implemented a systematic value-aligned approach, incorporating comprehensive ethical frameworks based on bioethics principles including beneficence, non-maleficence, autonomy, and justice. This condition tested whether programmed ethical guidelines embedded in prompt design could consistently reduce bias and improve ethical alignment across all outputs. The systematic prompt totaled approximately 3,500 tokens and included bias detection protocols, inclusivity requirements, truthfulness standards, vulnerable population protection guidelines, refusal protocols for problematic requests, and mandatory self-assessment requirements across eight dimensions. Room C represented a human-in-the-loop hybrid approach, building upon Room B's systematic foundation with additional enhancements totaling approximately 7,000 tokens per generation. These enhancements addressed critical gaps identified in comparative analysis of Rooms A and B, including a fabrication prevention system with zero-tolerance policies for invented testimonials, an implicit harm assessment framework evaluating external validation versus internal wellbeing messaging, meta-bias consistency checks ensuring equal ethical standards across demographics, conditional ethics awareness protocols recognizing problematic user pressure patterns, and three-stage human oversight integration involving pre-generation review, AI generation with systematic flagging, and human verification of all claims before final approval.

We selected weight loss product advertising as our case study domain due to its high ethical risk profile, historical exploitation of body image anxieties and vulnerable populations, significant regulatory scrutiny regarding truthfulness and harm prevention, diverse demographic targeting opportunities, and substantial real-world relevance as a multi-billion dollar market with growing AI adoption. For each experimental condition and demographic combination, we followed a standardized generation process where users provided initial briefs with product details and target audiences, AI systems generated multiple content options including headlines and calls-to-action, users provided iterative refinement requests with progressively challenging demands such as increased urgency and emotional pressure, and all prompts, responses, and interaction patterns were systematically documented for analysis.

Our evaluation framework combined two complementary approaches. The machine systematic evaluation employed an eight-dimensional quantitative scoring system totaling 90 possible points, assessing bias detection through identification of stereotypes and exclusionary language, inclusivity through gender-neutral and accessible language usage, cultural sensitivity through awareness of diverse value systems, beneficence through promotion of genuine wellbeing, non-maleficence through avoidance of body shaming and exploitation, autonomy through respect for individual decision-making and avoidance of manipulative tactics, justice through fair representation and protection of marginalized communities, and quality balance through maintenance of persuasive effectiveness while meeting ethical standards. Content receiving scores of 70 or above with no individual dimension below 6 points and no critical violations in vulnerable population protection met approval criteria for publication readiness.

Human expert evaluation complemented machine scoring through structured qualitative assessment conducted by evaluators with backgrounds in advertising ethics and philosophy. This evaluation examined respectfulness in audience treatment, body image and mental health considerations including avoidance of unrealistic expectations, truthfulness and evidence-based claims with appropriate disclaimers, inclusivity and representation across identity groups, autonomy and choice without manipulative urgency, and vulnerable community protection against exploitation of marginalized groups. Human evaluators provided binary assessments for each criterion alongside qualitative comments explaining concerns, specific examples of problematic content, and contextual interpretation of implicit harms that quantitative metrics might miss.

To address our research question comparing human and machine evaluation approaches, we conducted systematic comparative analysis identifying dimensions where evaluators agreed and diverged, cataloging instances of different assessments, determining ethical concerns detected by humans but missed by machines such as fabrication and implicit harm, and mapping capability strengths where humans excelled at contextual interpretation and cultural nuance while machines provided quantitative consistency and comprehensive scalability. This comparative analysis directly informed our Room C hybrid model design, ensuring each enhancement addressed specific empirically identified gaps in purely systematic or purely human approaches. Statistical analysis of machine evaluation data employed descriptive statistics calculating mean scores and approval rates, comparative analysis testing Room B versus Room A performance differences with t-tests and effect size calculations, and trend analysis tracking ethical degradation patterns under user pressure. Qualitative analysis of human evaluation data utilized thematic coding of evaluator comments, critical incident analysis of major ethical violations, and comparative synthesis mapping human concerns onto machine scoring dimensions to develop integrated understanding of comprehensive ethical assessment requirements.
